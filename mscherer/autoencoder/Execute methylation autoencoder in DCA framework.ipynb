{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-02 11:19:24.815540: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-02 11:19:24.815557: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/kopt/config.py:60: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  _config = yaml.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "from dca.api import dca\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d040f",
   "metadata": {},
   "source": [
    "### INPUT: Specify which sample you want to analyze. Currently available are:\n",
    "\n",
    "- Sample2_70_percent\n",
    "- Sample2_80_percent\n",
    "- Sample3_default\n",
    "- Sample4_70_percent\n",
    "- Sample4_80_percent\n",
    "- Sample5_70_percent\n",
    "- Sample5_80_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'Sample5_80_percent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cb355",
   "metadata": {},
   "source": [
    "### Loading the read count data and potentially removing cell doublets/mulitplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compressed-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_table('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/tsv/' + sample + '.barcode.cell.distribution.tsv',sep='\\t')\n",
    "remove_mixed = True\n",
    "if remove_mixed:\n",
    "    clust_file = pd.read_csv('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/'+ sample + '/tsv/cluster_assignment.tsv',\n",
    "                            sep='\\t')\n",
    "    drop_rows = clust_file['Barcode'][clust_file['CellType']=='Mixed']\n",
    "    dat = dat.drop(drop_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238c4e4",
   "metadata": {},
   "source": [
    "### Creating an AnnData object from the read counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(dat)\n",
    "sc.pp.filter_genes(adata,min_cells=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a670e",
   "metadata": {},
   "source": [
    "### Running the autoencoder: Currently, we have the following available architectures:\n",
    "\n",
    "- 'meth-encoder': A classical autoencoder, where the dispersion parameters of both NB-distributions can be freely selected by the model.\n",
    "- 'meth-encoder-constant': An autoencoder, where the dispersion parameters is fixed for each gene. This lowers the number of paramters substantially.\n",
    "- 'meth-encoder-poisson': An autoencoder, which mixes a negative bionomial distribution (foreground) with a Poisson distribution as the background\n",
    "- 'meth-encoder-poisson-constant': Same as the above, but with a fixed dispersion for each gene\n",
    "- 'meth-encoder-simple': Instead of having a dense layer representing the mean per cell and amplicon, we use a constant mean per amplicon for both the foreground and the background distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-02 11:19:26.762338: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-02 11:19:26.762485: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-02 11:19:26.762498: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-02 11:19:26.762514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (CZC041BTPK): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dca: Successfully preprocessed 199 genes and 5207 cells.\n",
      "WARNING:tensorflow:From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-02 11:19:26,948 [WARNING] From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "2021-08-02 11:19:26.949711: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              [(None, 199)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 16)           3200        count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           48          enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 8)            136         enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            24          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 16)           144         center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           48          dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean1 (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dispersion1 (ConstantDispersion (None, 199)          199         mean1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "alpha (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(None, 199)]        0           dispersion1[0][0]                \n",
      "                                                                 alpha[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 199)          0           dispersion1[0][0]                \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dispersion2 (ConstantDispersion (None, 199)          199         tf_op_layer_mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 199)          0           lambda_1[0][0]                   \n",
      "                                                                 pi[0][0]                         \n",
      "                                                                 alpha[0][0]                      \n",
      "                                                                 dispersion2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 14,147\n",
      "Trainable params: 14,067\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n",
      "Train on 4686 samples, validate on 521 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-02 11:19:27.840429: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-08-02 11:19:27.859988: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4686/4686 [==============================] - 1s 149us/sample - loss: 465.7851 - val_loss: 294.7353\n",
      "Epoch 2/300\n",
      "1152/4686 [======>.......................] - ETA: 0s - loss: 294.7743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4686/4686 [==============================] - 0s 44us/sample - loss: 221.6957 - val_loss: 162.3700\n",
      "Epoch 3/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 87.6008 - val_loss: 63.2342\n",
      "Epoch 4/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 26.7032 - val_loss: 14.7649\n",
      "Epoch 5/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 9.7909 - val_loss: 7.8985\n",
      "Epoch 6/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 7.4881 - val_loss: 7.2293\n",
      "Epoch 7/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 7.0559 - val_loss: 6.9814\n",
      "Epoch 8/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.8117 - val_loss: 6.7545\n",
      "Epoch 9/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.6561 - val_loss: 6.6137\n",
      "Epoch 10/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.5399 - val_loss: 6.5152\n",
      "Epoch 11/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.4559 - val_loss: 6.4428\n",
      "Epoch 12/300\n",
      "4686/4686 [==============================] - 0s 47us/sample - loss: 6.3926 - val_loss: 6.3917\n",
      "Epoch 13/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.3435 - val_loss: 6.3537\n",
      "Epoch 14/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.3149 - val_loss: 6.3281\n",
      "Epoch 15/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2960 - val_loss: 6.3147\n",
      "Epoch 16/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2821 - val_loss: 6.3019\n",
      "Epoch 17/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2745 - val_loss: 6.2960\n",
      "Epoch 18/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2661 - val_loss: 6.2939\n",
      "Epoch 19/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2617 - val_loss: 6.2872\n",
      "Epoch 20/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2589 - val_loss: 6.2830\n",
      "Epoch 21/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2548 - val_loss: 6.2833\n",
      "Epoch 22/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2522 - val_loss: 6.2804\n",
      "Epoch 23/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2505 - val_loss: 6.2787\n",
      "Epoch 24/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2487 - val_loss: 6.2790\n",
      "Epoch 25/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2485 - val_loss: 6.2776\n",
      "Epoch 26/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2461 - val_loss: 6.2767\n",
      "Epoch 27/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2482 - val_loss: 6.2738\n",
      "Epoch 28/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2435 - val_loss: 6.2763\n",
      "Epoch 29/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2459 - val_loss: 6.2746\n",
      "Epoch 30/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2438 - val_loss: 6.2763\n",
      "Epoch 31/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2440 - val_loss: 6.2733\n",
      "Epoch 32/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2418 - val_loss: 6.2730\n",
      "Epoch 33/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2412 - val_loss: 6.2739\n",
      "Epoch 34/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2419 - val_loss: 6.2720\n",
      "Epoch 35/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2392 - val_loss: 6.2712\n",
      "Epoch 36/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2404 - val_loss: 6.2714\n",
      "Epoch 37/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2395 - val_loss: 6.2714\n",
      "Epoch 38/300\n",
      "4686/4686 [==============================] - 0s 46us/sample - loss: 6.2389 - val_loss: 6.2735\n",
      "Epoch 39/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2395 - val_loss: 6.2713\n",
      "Epoch 40/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2390 - val_loss: 6.2697\n",
      "Epoch 41/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2378 - val_loss: 6.2712\n",
      "Epoch 42/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2374 - val_loss: 6.2712\n",
      "Epoch 43/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2372 - val_loss: 6.2699\n",
      "Epoch 44/300\n",
      "4686/4686 [==============================] - 0s 54us/sample - loss: 6.2358 - val_loss: 6.2707\n",
      "Epoch 45/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2351 - val_loss: 6.2692\n",
      "Epoch 46/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2374 - val_loss: 6.2699\n",
      "Epoch 47/300\n",
      "4686/4686 [==============================] - 0s 47us/sample - loss: 6.2365 - val_loss: 6.2696\n",
      "Epoch 48/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2341 - val_loss: 6.2694\n",
      "Epoch 49/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2357 - val_loss: 6.2680\n",
      "Epoch 50/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2353 - val_loss: 6.2691\n",
      "Epoch 51/300\n",
      "4686/4686 [==============================] - 0s 50us/sample - loss: 6.2350 - val_loss: 6.2660\n",
      "Epoch 52/300\n",
      "4686/4686 [==============================] - 0s 58us/sample - loss: 6.2349 - val_loss: 6.2686\n",
      "Epoch 53/300\n",
      "4686/4686 [==============================] - 0s 47us/sample - loss: 6.2348 - val_loss: 6.2657\n",
      "Epoch 54/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2353 - val_loss: 6.2677\n",
      "Epoch 55/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2347 - val_loss: 6.2679\n",
      "Epoch 56/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2333 - val_loss: 6.2688\n",
      "Epoch 57/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2353 - val_loss: 6.2689\n",
      "Epoch 58/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2359 - val_loss: 6.2675\n",
      "Epoch 59/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2324 - val_loss: 6.2683\n",
      "Epoch 60/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2328 - val_loss: 6.2684\n",
      "Epoch 61/300\n",
      "4686/4686 [==============================] - 0s 49us/sample - loss: 6.2328 - val_loss: 6.2677\n",
      "Epoch 62/300\n",
      "4686/4686 [==============================] - 0s 50us/sample - loss: 6.2327 - val_loss: 6.2700\n",
      "Epoch 63/300\n",
      "3712/4686 [======================>.......] - ETA: 0s - loss: 6.2297\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4686/4686 [==============================] - 0s 52us/sample - loss: 6.2323 - val_loss: 6.2689\n",
      "Epoch 64/300\n",
      "4686/4686 [==============================] - 0s 47us/sample - loss: 6.2299 - val_loss: 6.2670\n",
      "Epoch 65/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2310 - val_loss: 6.2670\n",
      "Epoch 66/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2290 - val_loss: 6.2669\n",
      "Epoch 67/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2295 - val_loss: 6.2670\n",
      "Epoch 68/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2291 - val_loss: 6.2666\n",
      "Epoch 69/300\n",
      "4686/4686 [==============================] - 0s 45us/sample - loss: 6.2280 - val_loss: 6.2668\n",
      "Epoch 70/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2285 - val_loss: 6.2669\n",
      "Epoch 71/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2274 - val_loss: 6.2672\n",
      "Epoch 72/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2303 - val_loss: 6.2671\n",
      "Epoch 73/300\n",
      "3872/4686 [=======================>......] - ETA: 0s - loss: 6.2280\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2278 - val_loss: 6.2671\n",
      "Epoch 74/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2298 - val_loss: 6.2672\n",
      "Epoch 75/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2294 - val_loss: 6.2670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2294 - val_loss: 6.2668\n",
      "Epoch 77/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2285 - val_loss: 6.2669\n",
      "Epoch 78/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2288 - val_loss: 6.2668\n",
      "Epoch 79/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2291 - val_loss: 6.2671\n",
      "Epoch 80/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2277 - val_loss: 6.2667\n",
      "Epoch 81/300\n",
      "4686/4686 [==============================] - 0s 52us/sample - loss: 6.2288 - val_loss: 6.2666\n",
      "Epoch 82/300\n",
      "4686/4686 [==============================] - 0s 50us/sample - loss: 6.2283 - val_loss: 6.2667\n",
      "Epoch 83/300\n",
      "3712/4686 [======================>.......] - ETA: 0s - loss: 6.2328\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2283 - val_loss: 6.2667\n",
      "Epoch 84/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2271 - val_loss: 6.2668\n",
      "Epoch 85/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2280 - val_loss: 6.2666\n",
      "Epoch 86/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2297 - val_loss: 6.2669\n",
      "Epoch 87/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2286 - val_loss: 6.2667\n",
      "Epoch 88/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2277 - val_loss: 6.2668\n",
      "Epoch 89/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2282 - val_loss: 6.2667\n",
      "Epoch 90/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2278 - val_loss: 6.2666\n",
      "Epoch 91/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2287 - val_loss: 6.2669\n",
      "Epoch 92/300\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2275 - val_loss: 6.2668\n",
      "Epoch 93/300\n",
      "3936/4686 [========================>.....] - ETA: 0s - loss: 6.2305\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "4686/4686 [==============================] - 0s 42us/sample - loss: 6.2275 - val_loss: 6.2669\n",
      "Epoch 94/300\n",
      "4686/4686 [==============================] - 0s 41us/sample - loss: 6.2282 - val_loss: 6.2669\n",
      "Epoch 95/300\n",
      "4686/4686 [==============================] - 0s 51us/sample - loss: 6.2296 - val_loss: 6.2670\n",
      "Epoch 96/300\n",
      "4686/4686 [==============================] - 0s 55us/sample - loss: 6.2286 - val_loss: 6.2667\n",
      "Epoch 97/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2279 - val_loss: 6.2668\n",
      "Epoch 98/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2289 - val_loss: 6.2668\n",
      "Epoch 99/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2274 - val_loss: 6.2668\n",
      "Epoch 100/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2281 - val_loss: 6.2668\n",
      "Epoch 101/300\n",
      "4686/4686 [==============================] - 0s 43us/sample - loss: 6.2285 - val_loss: 6.2668\n",
      "Epoch 102/300\n",
      "4686/4686 [==============================] - 0s 44us/sample - loss: 6.2293 - val_loss: 6.2668\n",
      "Epoch 103/300\n",
      "4320/4686 [==========================>...] - ETA: 0s - loss: 6.2349\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "4686/4686 [==============================] - 0s 52us/sample - loss: 6.2292 - val_loss: 6.2670\n",
      "Epoch 00103: early stopping\n"
     ]
    }
   ],
   "source": [
    "res = dca(adata,\n",
    "          ae_type='meth-encoder-constant',\n",
    "          return_info=True,\n",
    "          return_model=True,\n",
    "          return_bottleneck=True,\n",
    "          verbose=True,\n",
    "          early_stop=50,\n",
    "          network_kwds={'debug': True},\n",
    "          init='glorot_normal',\n",
    "          hidden_size=(16,8,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8237a8",
   "metadata": {},
   "source": [
    "### Writing the data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = '/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/methylation_autoencoder/'\n",
    "if not os.path.isdir(di):\n",
    "    os.mkdir(di)\n",
    "    \n",
    "pd.DataFrame(adata.obsm['X_meth_value']).to_csv(di + 'mixture_prob_dca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81520432",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = res.get_encoder().predict([adata.X, adata.X.mean(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb56ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata.obsm['X_bottleneck']).to_csv(di + 'bottleneck.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
