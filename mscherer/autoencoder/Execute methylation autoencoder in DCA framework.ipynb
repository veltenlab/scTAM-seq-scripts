{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 10:36:13.730703: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-19 10:36:13.730721: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/kopt/config.py:60: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  _config = yaml.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "from dca.api import dca\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d040f",
   "metadata": {},
   "source": [
    "### INPUT: Specify which sample you want to analyze. Currently available are:\n",
    "\n",
    "- Sample2_70_percent\n",
    "- Sample2_80_percent\n",
    "- Sample3_default\n",
    "- Sample4_70_percent\n",
    "- Sample4_80_percent\n",
    "- Sample5_70_percent\n",
    "- Sample5_80_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'Sample5_80_percent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cb355",
   "metadata": {},
   "source": [
    "### Loading the read count data and potentially removing cell doublets/mulitplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compressed-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_table('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/tsv/' + sample + '.barcode.cell.distribution.tsv',sep='\\t')\n",
    "remove_mixed = False\n",
    "if remove_mixed:\n",
    "    clust_file = pd.read_csv('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/'+ sample + '/tsv/cluster_assignment.tsv',\n",
    "                            sep='\\t')\n",
    "    drop_rows = clust_file['Barcode'][clust_file['CellType']=='Mixed']\n",
    "    dat = dat.drop(drop_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238c4e4",
   "metadata": {},
   "source": [
    "### Creating an AnnData object from the read counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(dat)\n",
    "sc.pp.filter_genes(adata,min_cells=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a670e",
   "metadata": {},
   "source": [
    "### Running the autoencoder: Currently, we have the following available architectures:\n",
    "\n",
    "- 'meth-encoder': A classical autoencoder, where the dispersion parameters of both NB-distributions can be freely selected by the model.\n",
    "- 'meth-encoder-constant': An autoencoder, where the dispersion parameters is fixed for each gene. This lowers the number of paramters substantially.\n",
    "- 'meth-encoder-poisson': An autoencoder, which mixes a negative bionomial distribution (foreground) with a Poisson distribution as the background\n",
    "- 'meth-encoder-poisson-constant': Same as the above, but with a fixed dispersion for each gene\n",
    "- 'meth-encoder-simple': Instead of having a dense layer representing the mean per cell and amplicon, we use a constant mean per amplicon for both the foreground and the background distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 10:36:15.619300: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-19 10:36:15.619445: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-19 10:36:15.619457: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-19 10:36:15.619474: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (CZC041BTPK): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dca: Successfully preprocessed 199 genes and 5517 cells.\n",
      "WARNING:tensorflow:From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 10:36:15,840 [WARNING] From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "2021-08-19 10:36:15.842868: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              [(None, 199)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 16)           3200        count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           48          enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 8)            136         enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            24          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 16)           144         center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           48          dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enzyme_cells (Linear)           (None, 1)            17          dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 199)          0           pi[0][0]                         \n",
      "                                                                 enzyme_cells[0][0]               \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mean1 (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dispersion1 (Dense)             (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dispersion2 (Dense)             (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "alpha (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 199)          0           lambda_1[0][0]                   \n",
      "                                                                 mean1[0][0]                      \n",
      "                                                                 dispersion1[0][0]                \n",
      "                                                                 dispersion2[0][0]                \n",
      "                                                                 alpha[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 20,532\n",
      "Trainable params: 20,452\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n",
      "Train on 4965 samples, validate on 552 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-19 10:36:16.764714: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-08-19 10:36:16.789078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4965/4965 [==============================] - 1s 179us/sample - loss: 425.3357 - val_loss: 223.8962\n",
      "Epoch 2/300\n",
      "  32/4965 [..............................] - ETA: 0s - loss: 189.7806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4965/4965 [==============================] - 0s 70us/sample - loss: 100.1812 - val_loss: 41.9903\n",
      "Epoch 3/300\n",
      "4965/4965 [==============================] - 0s 74us/sample - loss: 17.1962 - val_loss: 9.8481\n",
      "Epoch 4/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 8.9905 - val_loss: 8.2929\n",
      "Epoch 5/300\n",
      "4965/4965 [==============================] - 0s 66us/sample - loss: 7.8323 - val_loss: 7.3660\n",
      "Epoch 6/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 7.1487 - val_loss: 6.9820\n",
      "Epoch 7/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.9271 - val_loss: 6.8784\n",
      "Epoch 8/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.8772 - val_loss: 6.8588\n",
      "Epoch 9/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8655 - val_loss: 6.8488\n",
      "Epoch 10/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8571 - val_loss: 6.8401\n",
      "Epoch 11/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.8507 - val_loss: 6.8319\n",
      "Epoch 12/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8401 - val_loss: 6.8223\n",
      "Epoch 13/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.8318 - val_loss: 6.8158\n",
      "Epoch 14/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.8240 - val_loss: 6.8120\n",
      "Epoch 15/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.8221 - val_loss: 6.8093\n",
      "Epoch 16/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.8194 - val_loss: 6.8076\n",
      "Epoch 17/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8179 - val_loss: 6.8068\n",
      "Epoch 18/300\n",
      "4965/4965 [==============================] - 0s 68us/sample - loss: 6.8187 - val_loss: 6.8046\n",
      "Epoch 19/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.8170 - val_loss: 6.8033\n",
      "Epoch 20/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8162 - val_loss: 6.8040\n",
      "Epoch 21/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.8150 - val_loss: 6.8021\n",
      "Epoch 22/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.8142 - val_loss: 6.8018\n",
      "Epoch 23/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8148 - val_loss: 6.8006\n",
      "Epoch 24/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8135 - val_loss: 6.8005\n",
      "Epoch 25/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.8128 - val_loss: 6.7997\n",
      "Epoch 26/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8123 - val_loss: 6.8000\n",
      "Epoch 27/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.8121 - val_loss: 6.8003\n",
      "Epoch 28/300\n",
      "4965/4965 [==============================] - 0s 71us/sample - loss: 6.8119 - val_loss: 6.7998\n",
      "Epoch 29/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8118 - val_loss: 6.7982\n",
      "Epoch 30/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8115 - val_loss: 6.7991\n",
      "Epoch 31/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.8092 - val_loss: 6.7972\n",
      "Epoch 32/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.8099 - val_loss: 6.7985\n",
      "Epoch 33/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.8108 - val_loss: 6.7970\n",
      "Epoch 34/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8096 - val_loss: 6.7962\n",
      "Epoch 35/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.8100 - val_loss: 6.7973\n",
      "Epoch 36/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8071 - val_loss: 6.7963\n",
      "Epoch 37/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8074 - val_loss: 6.7956\n",
      "Epoch 38/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8073 - val_loss: 6.7952\n",
      "Epoch 39/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.8067 - val_loss: 6.7956\n",
      "Epoch 40/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.8071 - val_loss: 6.7948\n",
      "Epoch 41/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.8068 - val_loss: 6.7946\n",
      "Epoch 42/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.8058 - val_loss: 6.7941\n",
      "Epoch 43/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.8050 - val_loss: 6.7929\n",
      "Epoch 44/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.8038 - val_loss: 6.7927\n",
      "Epoch 45/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.8047 - val_loss: 6.7952\n",
      "Epoch 46/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.8037 - val_loss: 6.7936\n",
      "Epoch 47/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.8044 - val_loss: 6.7914\n",
      "Epoch 48/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.8026 - val_loss: 6.7908\n",
      "Epoch 49/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8028 - val_loss: 6.7901\n",
      "Epoch 50/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.8030 - val_loss: 6.7909\n",
      "Epoch 51/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.8016 - val_loss: 6.7922\n",
      "Epoch 52/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8029 - val_loss: 6.7907\n",
      "Epoch 53/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.8019 - val_loss: 6.7893\n",
      "Epoch 54/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.8011 - val_loss: 6.7896\n",
      "Epoch 55/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.8021 - val_loss: 6.7906\n",
      "Epoch 56/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.8000 - val_loss: 6.7890\n",
      "Epoch 57/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7998 - val_loss: 6.7888\n",
      "Epoch 58/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7998 - val_loss: 6.7889\n",
      "Epoch 59/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.8010 - val_loss: 6.7880\n",
      "Epoch 60/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.8001 - val_loss: 6.7870\n",
      "Epoch 61/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.8007 - val_loss: 6.7884\n",
      "Epoch 62/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7984 - val_loss: 6.7874\n",
      "Epoch 63/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7987 - val_loss: 6.7883\n",
      "Epoch 64/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7996 - val_loss: 6.7893\n",
      "Epoch 65/300\n",
      "4965/4965 [==============================] - 0s 72us/sample - loss: 6.7992 - val_loss: 6.7878\n",
      "Epoch 66/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7990 - val_loss: 6.7885\n",
      "Epoch 67/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7981 - val_loss: 6.7876\n",
      "Epoch 68/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7983 - val_loss: 6.7857\n",
      "Epoch 69/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7978 - val_loss: 6.7872\n",
      "Epoch 70/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7981 - val_loss: 6.7853\n",
      "Epoch 71/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7977 - val_loss: 6.7858\n",
      "Epoch 72/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7986 - val_loss: 6.7872\n",
      "Epoch 73/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7973 - val_loss: 6.7858\n",
      "Epoch 74/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7971 - val_loss: 6.7855\n",
      "Epoch 75/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7977 - val_loss: 6.7867\n",
      "Epoch 76/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7959 - val_loss: 6.7844\n",
      "Epoch 77/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7976 - val_loss: 6.7862\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7953 - val_loss: 6.7858\n",
      "Epoch 79/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7956 - val_loss: 6.7853\n",
      "Epoch 80/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7950 - val_loss: 6.7850\n",
      "Epoch 81/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7951 - val_loss: 6.7847\n",
      "Epoch 82/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7963 - val_loss: 6.7844\n",
      "Epoch 83/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7950 - val_loss: 6.7848\n",
      "Epoch 84/300\n",
      "4965/4965 [==============================] - 0s 53us/sample - loss: 6.7941 - val_loss: 6.7832\n",
      "Epoch 85/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7943 - val_loss: 6.7847\n",
      "Epoch 86/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7943 - val_loss: 6.7837\n",
      "Epoch 87/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7949 - val_loss: 6.7829\n",
      "Epoch 88/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7927 - val_loss: 6.7832\n",
      "Epoch 89/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7929 - val_loss: 6.7818\n",
      "Epoch 90/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7926 - val_loss: 6.7829\n",
      "Epoch 91/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7927 - val_loss: 6.7830\n",
      "Epoch 92/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7914 - val_loss: 6.7808\n",
      "Epoch 93/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7939 - val_loss: 6.7839\n",
      "Epoch 94/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7914 - val_loss: 6.7827\n",
      "Epoch 95/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7925 - val_loss: 6.7826\n",
      "Epoch 96/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7914 - val_loss: 6.7809\n",
      "Epoch 97/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7904 - val_loss: 6.7807\n",
      "Epoch 98/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7906 - val_loss: 6.7800\n",
      "Epoch 99/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7907 - val_loss: 6.7808\n",
      "Epoch 100/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7903 - val_loss: 6.7804\n",
      "Epoch 101/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7892 - val_loss: 6.7805\n",
      "Epoch 102/300\n",
      "4965/4965 [==============================] - 0s 53us/sample - loss: 6.7896 - val_loss: 6.7808\n",
      "Epoch 103/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7908 - val_loss: 6.7797\n",
      "Epoch 104/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7902 - val_loss: 6.7791\n",
      "Epoch 105/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7888 - val_loss: 6.7803\n",
      "Epoch 106/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7888 - val_loss: 6.7795\n",
      "Epoch 107/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7900 - val_loss: 6.7785\n",
      "Epoch 108/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7893 - val_loss: 6.7782\n",
      "Epoch 109/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7879 - val_loss: 6.7816\n",
      "Epoch 110/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7881 - val_loss: 6.7787\n",
      "Epoch 111/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7890 - val_loss: 6.7809\n",
      "Epoch 112/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7885 - val_loss: 6.7794\n",
      "Epoch 113/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7876 - val_loss: 6.7800\n",
      "Epoch 114/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7882 - val_loss: 6.7782\n",
      "Epoch 115/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7875 - val_loss: 6.7788\n",
      "Epoch 116/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7871 - val_loss: 6.7797\n",
      "Epoch 117/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7882 - val_loss: 6.7776\n",
      "Epoch 118/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7872 - val_loss: 6.7779\n",
      "Epoch 119/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7867 - val_loss: 6.7780\n",
      "Epoch 120/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7865 - val_loss: 6.7783\n",
      "Epoch 121/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7869 - val_loss: 6.7786\n",
      "Epoch 122/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7858 - val_loss: 6.7791\n",
      "Epoch 123/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7875 - val_loss: 6.7773\n",
      "Epoch 124/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7857 - val_loss: 6.7775\n",
      "Epoch 125/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7862 - val_loss: 6.7781\n",
      "Epoch 126/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7861 - val_loss: 6.7771\n",
      "Epoch 127/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7851 - val_loss: 6.7770\n",
      "Epoch 128/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7864 - val_loss: 6.7783\n",
      "Epoch 129/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7868 - val_loss: 6.7777\n",
      "Epoch 130/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7849 - val_loss: 6.7773\n",
      "Epoch 131/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7864 - val_loss: 6.7767\n",
      "Epoch 132/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7847 - val_loss: 6.7774\n",
      "Epoch 133/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7860 - val_loss: 6.7792\n",
      "Epoch 134/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7862 - val_loss: 6.7777\n",
      "Epoch 135/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7850 - val_loss: 6.7773\n",
      "Epoch 136/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7853 - val_loss: 6.7766\n",
      "Epoch 137/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7846 - val_loss: 6.7759\n",
      "Epoch 138/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7860 - val_loss: 6.7774\n",
      "Epoch 139/300\n",
      "4965/4965 [==============================] - 0s 71us/sample - loss: 6.7846 - val_loss: 6.7766\n",
      "Epoch 140/300\n",
      "4965/4965 [==============================] - 0s 75us/sample - loss: 6.7845 - val_loss: 6.7764\n",
      "Epoch 141/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7854 - val_loss: 6.7759\n",
      "Epoch 142/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7834 - val_loss: 6.7776\n",
      "Epoch 143/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7850 - val_loss: 6.7761\n",
      "Epoch 144/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7835 - val_loss: 6.7795\n",
      "Epoch 145/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7833 - val_loss: 6.7761\n",
      "Epoch 146/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7836 - val_loss: 6.7768\n",
      "Epoch 147/300\n",
      "4192/4965 [========================>.....] - ETA: 0s - loss: 6.7837\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4965/4965 [==============================] - 0s 71us/sample - loss: 6.7841 - val_loss: 6.7767\n",
      "Epoch 148/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7833 - val_loss: 6.7723\n",
      "Epoch 149/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7833 - val_loss: 6.7721\n",
      "Epoch 150/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7818 - val_loss: 6.7721\n",
      "Epoch 151/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7814 - val_loss: 6.7720\n",
      "Epoch 152/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7820 - val_loss: 6.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7807 - val_loss: 6.7719\n",
      "Epoch 154/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7823 - val_loss: 6.7720\n",
      "Epoch 155/300\n",
      "4965/4965 [==============================] - 0s 70us/sample - loss: 6.7827 - val_loss: 6.7720\n",
      "Epoch 156/300\n",
      "4965/4965 [==============================] - 0s 66us/sample - loss: 6.7809 - val_loss: 6.7719\n",
      "Epoch 157/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7812 - val_loss: 6.7719\n",
      "Epoch 158/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7804 - val_loss: 6.7719\n",
      "Epoch 159/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7814 - val_loss: 6.7718\n",
      "Epoch 160/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7812 - val_loss: 6.7719\n",
      "Epoch 161/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7824 - val_loss: 6.7719\n",
      "Epoch 162/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7816 - val_loss: 6.7719\n",
      "Epoch 163/300\n",
      "4256/4965 [========================>.....] - ETA: 0s - loss: 6.7872\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4965/4965 [==============================] - 0s 67us/sample - loss: 6.7811 - val_loss: 6.7719\n",
      "Epoch 164/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7811 - val_loss: 6.7718\n",
      "Epoch 165/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7796 - val_loss: 6.7718\n",
      "Epoch 166/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7807 - val_loss: 6.7718\n",
      "Epoch 167/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7812 - val_loss: 6.7717\n",
      "Epoch 168/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 169/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 170/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7807 - val_loss: 6.7717\n",
      "Epoch 171/300\n",
      "4965/4965 [==============================] - 0s 68us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 172/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7804 - val_loss: 6.7717\n",
      "Epoch 173/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7814 - val_loss: 6.7717\n",
      "Epoch 174/300\n",
      "4736/4965 [===========================>..] - ETA: 0s - loss: 6.7752\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "4965/4965 [==============================] - 0s 69us/sample - loss: 6.7824 - val_loss: 6.7718\n",
      "Epoch 175/300\n",
      "4965/4965 [==============================] - 0s 74us/sample - loss: 6.7821 - val_loss: 6.7717\n",
      "Epoch 176/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7813 - val_loss: 6.7718\n",
      "Epoch 177/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7811 - val_loss: 6.7717\n",
      "Epoch 178/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 179/300\n",
      "4965/4965 [==============================] - 0s 67us/sample - loss: 6.7796 - val_loss: 6.7717\n",
      "Epoch 180/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7808 - val_loss: 6.7718\n",
      "Epoch 181/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7793 - val_loss: 6.7717\n",
      "Epoch 182/300\n",
      "4965/4965 [==============================] - 0s 71us/sample - loss: 6.7804 - val_loss: 6.7717\n",
      "Epoch 183/300\n",
      "4965/4965 [==============================] - 0s 59us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 184/300\n",
      "4608/4965 [==========================>...] - ETA: 0s - loss: 6.7873\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7812 - val_loss: 6.7717\n",
      "Epoch 185/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7814 - val_loss: 6.7717\n",
      "Epoch 186/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7816 - val_loss: 6.7717\n",
      "Epoch 187/300\n",
      "4965/4965 [==============================] - 0s 72us/sample - loss: 6.7817 - val_loss: 6.7717\n",
      "Epoch 188/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7816 - val_loss: 6.7717\n",
      "Epoch 189/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 190/300\n",
      "4965/4965 [==============================] - 0s 68us/sample - loss: 6.7798 - val_loss: 6.7717\n",
      "Epoch 191/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7825 - val_loss: 6.7718\n",
      "Epoch 192/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7819 - val_loss: 6.7718\n",
      "Epoch 193/300\n",
      "4965/4965 [==============================] - 0s 55us/sample - loss: 6.7811 - val_loss: 6.7718\n",
      "Epoch 194/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7821 - val_loss: 6.7719\n",
      "Epoch 195/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7802 - val_loss: 6.7717\n",
      "Epoch 196/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7819 - val_loss: 6.7717\n",
      "Epoch 197/300\n",
      "4965/4965 [==============================] - 0s 69us/sample - loss: 6.7825 - val_loss: 6.7718\n",
      "Epoch 198/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 199/300\n",
      "4965/4965 [==============================] - 0s 71us/sample - loss: 6.7826 - val_loss: 6.7718\n",
      "Epoch 200/300\n",
      "4512/4965 [==========================>...] - ETA: 0s - loss: 6.7804\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.7803 - val_loss: 6.7718\n",
      "Epoch 201/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7808 - val_loss: 6.7718\n",
      "Epoch 202/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 203/300\n",
      "4965/4965 [==============================] - 0s 70us/sample - loss: 6.7810 - val_loss: 6.7717\n",
      "Epoch 204/300\n",
      "4965/4965 [==============================] - 0s 62us/sample - loss: 6.7810 - val_loss: 6.7717\n",
      "Epoch 205/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7819 - val_loss: 6.7718\n",
      "Epoch 206/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7811 - val_loss: 6.7718\n",
      "Epoch 207/300\n",
      "4965/4965 [==============================] - 0s 64us/sample - loss: 6.7812 - val_loss: 6.7718\n",
      "Epoch 208/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7809 - val_loss: 6.7718\n",
      "Epoch 209/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7805 - val_loss: 6.7717\n",
      "Epoch 210/300\n",
      "4768/4965 [===========================>..] - ETA: 0s - loss: 6.7860\n",
      "Epoch 00210: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7818 - val_loss: 6.7718\n",
      "Epoch 211/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7815 - val_loss: 6.7719\n",
      "Epoch 212/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7808 - val_loss: 6.7718\n",
      "Epoch 213/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7805 - val_loss: 6.7718\n",
      "Epoch 214/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7813 - val_loss: 6.7717\n",
      "Epoch 215/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7808 - val_loss: 6.7717\n",
      "Epoch 216/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7813 - val_loss: 6.7718\n",
      "Epoch 217/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7817 - val_loss: 6.7717\n",
      "Epoch 218/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7818 - val_loss: 6.7718\n",
      "Epoch 219/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7818 - val_loss: 6.7717\n",
      "Epoch 220/300\n",
      "4896/4965 [============================>.] - ETA: 0s - loss: 6.7800\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7814 - val_loss: 6.7719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7809 - val_loss: 6.7718\n",
      "Epoch 222/300\n",
      "4965/4965 [==============================] - 0s 61us/sample - loss: 6.7826 - val_loss: 6.7717\n",
      "Epoch 223/300\n",
      "4965/4965 [==============================] - 0s 54us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 224/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7801 - val_loss: 6.7717\n",
      "Epoch 225/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7815 - val_loss: 6.7717\n",
      "Epoch 226/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7806 - val_loss: 6.7717\n",
      "Epoch 227/300\n",
      "4965/4965 [==============================] - 0s 58us/sample - loss: 6.7807 - val_loss: 6.7717\n",
      "Epoch 228/300\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7821 - val_loss: 6.7717\n",
      "Epoch 229/300\n",
      "4965/4965 [==============================] - 0s 56us/sample - loss: 6.7814 - val_loss: 6.7718\n",
      "Epoch 230/300\n",
      "4832/4965 [============================>.] - ETA: 0s - loss: 6.7766\n",
      "Epoch 00230: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "4965/4965 [==============================] - 0s 57us/sample - loss: 6.7803 - val_loss: 6.7717\n",
      "Epoch 231/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7811 - val_loss: 6.7718\n",
      "Epoch 232/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7809 - val_loss: 6.7718\n",
      "Epoch 233/300\n",
      "4965/4965 [==============================] - 0s 67us/sample - loss: 6.7805 - val_loss: 6.7717\n",
      "Epoch 234/300\n",
      "4965/4965 [==============================] - 0s 68us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 235/300\n",
      "4965/4965 [==============================] - 0s 63us/sample - loss: 6.7801 - val_loss: 6.7717\n",
      "Epoch 236/300\n",
      "4965/4965 [==============================] - 0s 70us/sample - loss: 6.7810 - val_loss: 6.7717\n",
      "Epoch 237/300\n",
      "4965/4965 [==============================] - 0s 69us/sample - loss: 6.7813 - val_loss: 6.7717\n",
      "Epoch 238/300\n",
      "4965/4965 [==============================] - 0s 60us/sample - loss: 6.7810 - val_loss: 6.7718\n",
      "Epoch 239/300\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7810 - val_loss: 6.7717\n",
      "Epoch 240/300\n",
      "4224/4965 [========================>.....] - ETA: 0s - loss: 6.7800\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "4965/4965 [==============================] - 0s 65us/sample - loss: 6.7809 - val_loss: 6.7717\n",
      "Epoch 00240: early stopping\n",
      "dca: Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "res = dca(adata,\n",
    "          ae_type='meth-encoder',\n",
    "          return_info=True,\n",
    "          return_model=True,\n",
    "          return_bottleneck=True,\n",
    "          verbose=True,\n",
    "          early_stop=50,\n",
    "          epochs=300,\n",
    "          network_kwds={'debug': True},\n",
    "          init='glorot_normal',\n",
    "          hidden_size=(16,8,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8237a8",
   "metadata": {},
   "source": [
    "### Writing the data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = '/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/methylation_autoencoder/'\n",
    "if not os.path.isdir(di):\n",
    "    os.mkdir(di)\n",
    "    \n",
    "pd.DataFrame(adata.obsm['X_meth_value']).to_csv(di + 'mixture_prob_dca_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81520432",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = res.get_encoder().predict([adata.X, adata.X.mean(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb56ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata.obsm['X_bottleneck']).to_csv(di + 'bottleneck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62af1a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3201.415 , 33807.57  , 47586.242 , ..., 16341.072 , 12737.987 ,\n",
       "         7052.2705],\n",
       "       [ 6809.6797, 34775.082 , 23742.154 , ...,  8528.204 ,  4665.371 ,\n",
       "        10822.638 ],\n",
       "       [ 3797.4695, 37645.824 , 36656.6   , ..., 12467.072 , 12121.334 ,\n",
       "        17415.514 ],\n",
       "       ...,\n",
       "       [11586.969 , 45598.39  , 36234.965 , ..., 14381.625 , 20681.67  ,\n",
       "        22036.402 ],\n",
       "       [ 5797.2627, 27189.355 , 20298.021 , ...,  7200.931 ,  3962.354 ,\n",
       "         8660.505 ],\n",
       "       [ 6482.4053, 39885.234 , 25736.547 , ...,  6097.3853,  6350.377 ,\n",
       "        12610.674 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obsm['mean1_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c5afa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mean2_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6980/297025524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean2_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/anndata/_core/aligned_mapping.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mean2_norm'"
     ]
    }
   ],
   "source": [
    "adata.obsm['mean2_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2377f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['X_enzyme_activity'].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
