{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:12:27.613763: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-17 14:12:27.613780: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/kopt/config.py:60: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  _config = yaml.load(open(_config_path))\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "from dca.api import dca\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d040f",
   "metadata": {},
   "source": [
    "### INPUT: Specify which sample you want to analyze. Currently available are:\n",
    "\n",
    "- Sample2_70_percent\n",
    "- Sample2_80_percent\n",
    "- Sample3_default\n",
    "- Sample4_70_percent\n",
    "- Sample4_80_percent\n",
    "- Sample5_70_percent\n",
    "- Sample5_80_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7f70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aquatic-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'Sample5_80_percent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cb355",
   "metadata": {},
   "source": [
    "### Loading the read count data and potentially removing cell doublets/mulitplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compressed-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat = pd.read_table('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/tsv/' + sample + '.barcode.cell.distribution.tsv',sep='\\t')\n",
    "dat = pd.read_table('/users/lvelten/project/Methylome/analysis/missionbio/downsampling/Sample5_70_percent/tsv/Sample5_70_percent.barcode.cell.distribution.tsv',sep='\\t')\n",
    "#dat = (dat*0.1).round()\n",
    "remove_mixed = True\n",
    "if remove_mixed:\n",
    "    #clust_file = pd.read_csv('/users/lvelten/project/Methylome/analysis/missionbio/tapestri/'+ sample + '/tsv/cluster_assignment.tsv',\n",
    "    #                        sep='\\t')\n",
    "    clust_file = pd.read_csv('/users/lvelten/project/Methylome/analysis/missionbio/downsampling/Sample5/percent_1/tsv/cluster_assignment.tsv',\n",
    "                            sep='\\t')\n",
    "    drop_rows = clust_file['Barcode'][clust_file['CellType']=='Mixed']\n",
    "    dat = dat.drop(drop_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238c4e4",
   "metadata": {},
   "source": [
    "### Creating an AnnData object from the read counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(dat)\n",
    "sc.pp.filter_genes(adata,min_cells=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a670e",
   "metadata": {},
   "source": [
    "### Running the autoencoder: Currently, we have the following available architectures:\n",
    "\n",
    "- 'meth-encoder': A classical autoencoder, where the dispersion parameters of both NB-distributions can be freely selected by the model.\n",
    "- 'meth-encoder-constant': An autoencoder, where the dispersion parameters is fixed for each gene. This lowers the number of paramters substantially.\n",
    "- 'meth-encoder-poisson': An autoencoder, which mixes a negative bionomial distribution (foreground) with a Poisson distribution as the background\n",
    "- 'meth-encoder-poisson-constant': Same as the above, but with a fixed dispersion for each gene\n",
    "- 'meth-encoder-simple': Instead of having a dense layer representing the mean per cell and amplicon, we use a constant mean per amplicon for both the foreground and the background distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elementary-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dca: Successfully preprocessed 199 genes and 1720 cells.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:12:29.819020: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-17 14:12:29.819153: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-17 14:12:29.819163: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-17 14:12:29.819178: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (CZC041BTPK): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:12:30,022 [WARNING] From /users/lvelten/project/Methylome/src/error_mod/dca/dca/train.py:41: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "2021-09-17 14:12:30.024122: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              [(None, 199)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 16)           3200        count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           48          enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 8)            136         enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            24          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 16)           144         center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           48          dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean1 (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dispersion1 (ConstantDispersion (None, 199)          199         mean1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "alpha (Dense)                   (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 199)          3383        dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enzyme_cells (Linear)           (None, 1)            17          dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(None, 199)]        0           dispersion1[0][0]                \n",
      "                                                                 alpha[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 199)          0           pi[0][0]                         \n",
      "                                                                 enzyme_cells[0][0]               \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion2 (ConstantDispersion (None, 199)          199         tf_op_layer_mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 199)          0           lambda_1[0][0]                   \n",
      "                                                                 dispersion1[0][0]                \n",
      "                                                                 alpha[0][0]                      \n",
      "                                                                 dispersion2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 14,164\n",
      "Trainable params: 14,084\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1548 samples, validate on 172 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 14:12:30.906059: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-09-17 14:12:30.928464: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548/1548 [==============================] - 1s 381us/sample - loss: 124.6694 - val_loss: 113.4408\n",
      "Epoch 2/300\n",
      "1548/1548 [==============================] - 0s 38us/sample - loss: 103.4664 - val_loss: 89.6890\n",
      "Epoch 3/300\n",
      "  32/1548 [..............................] - ETA: 0s - loss: 87.2033"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mscherer/software/anaconda3/envs/dca/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548/1548 [==============================] - 0s 41us/sample - loss: 83.9849 - val_loss: 68.6363\n",
      "Epoch 4/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 66.9919 - val_loss: 51.7414\n",
      "Epoch 5/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 52.1279 - val_loss: 40.1261\n",
      "Epoch 6/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 39.1317 - val_loss: 31.6256\n",
      "Epoch 7/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 28.3987 - val_loss: 24.5974\n",
      "Epoch 8/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 20.2949 - val_loss: 18.5912\n",
      "Epoch 9/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 14.4283 - val_loss: 13.9890\n",
      "Epoch 10/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 10.5431 - val_loss: 10.3315\n",
      "Epoch 11/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 8.4493 - val_loss: 8.2764\n",
      "Epoch 12/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 7.2608 - val_loss: 7.2566\n",
      "Epoch 13/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 6.7900 - val_loss: 6.7870\n",
      "Epoch 14/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 6.5993 - val_loss: 6.6099\n",
      "Epoch 15/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 6.4834 - val_loss: 6.5018\n",
      "Epoch 16/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 6.4037 - val_loss: 6.4455\n",
      "Epoch 17/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 6.3424 - val_loss: 6.3695\n",
      "Epoch 18/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 6.2810 - val_loss: 6.3205\n",
      "Epoch 19/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 6.2329 - val_loss: 6.2859\n",
      "Epoch 20/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 6.1803 - val_loss: 6.2113\n",
      "Epoch 21/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 6.1212 - val_loss: 6.1737\n",
      "Epoch 22/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 6.0720 - val_loss: 6.1202\n",
      "Epoch 23/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 6.0239 - val_loss: 6.0775\n",
      "Epoch 24/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.9861 - val_loss: 6.0449\n",
      "Epoch 25/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.9534 - val_loss: 6.0129\n",
      "Epoch 26/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.9248 - val_loss: 5.9781\n",
      "Epoch 27/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.8941 - val_loss: 5.9545\n",
      "Epoch 28/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.8703 - val_loss: 5.9362\n",
      "Epoch 29/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.8504 - val_loss: 5.9113\n",
      "Epoch 30/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.8268 - val_loss: 5.8871\n",
      "Epoch 31/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.8084 - val_loss: 5.8718\n",
      "Epoch 32/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.7906 - val_loss: 5.8577\n",
      "Epoch 33/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.7751 - val_loss: 5.8473\n",
      "Epoch 34/300\n",
      "1548/1548 [==============================] - 0s 53us/sample - loss: 5.7625 - val_loss: 5.8319\n",
      "Epoch 35/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.7469 - val_loss: 5.8210\n",
      "Epoch 36/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.7412 - val_loss: 5.8181\n",
      "Epoch 37/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.7285 - val_loss: 5.8034\n",
      "Epoch 38/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.7190 - val_loss: 5.7924\n",
      "Epoch 39/300\n",
      "1548/1548 [==============================] - 0s 59us/sample - loss: 5.7103 - val_loss: 5.7875\n",
      "Epoch 40/300\n",
      "1548/1548 [==============================] - 0s 70us/sample - loss: 5.7036 - val_loss: 5.7768\n",
      "Epoch 41/300\n",
      "1548/1548 [==============================] - 0s 57us/sample - loss: 5.6989 - val_loss: 5.7748\n",
      "Epoch 42/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6932 - val_loss: 5.7654\n",
      "Epoch 43/300\n",
      "1548/1548 [==============================] - 0s 55us/sample - loss: 5.6887 - val_loss: 5.7636\n",
      "Epoch 44/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6856 - val_loss: 5.7602\n",
      "Epoch 45/300\n",
      "1548/1548 [==============================] - 0s 56us/sample - loss: 5.6823 - val_loss: 5.7555\n",
      "Epoch 46/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6783 - val_loss: 5.7527\n",
      "Epoch 47/300\n",
      "1548/1548 [==============================] - 0s 56us/sample - loss: 5.6795 - val_loss: 5.7523\n",
      "Epoch 48/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6767 - val_loss: 5.7500\n",
      "Epoch 49/300\n",
      "1548/1548 [==============================] - 0s 54us/sample - loss: 5.6733 - val_loss: 5.7462\n",
      "Epoch 50/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6708 - val_loss: 5.7463\n",
      "Epoch 51/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.6699 - val_loss: 5.7420\n",
      "Epoch 52/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.6664 - val_loss: 5.7413\n",
      "Epoch 53/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6665 - val_loss: 5.7421\n",
      "Epoch 54/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6654 - val_loss: 5.7399\n",
      "Epoch 55/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6646 - val_loss: 5.7386\n",
      "Epoch 56/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6655 - val_loss: 5.7407\n",
      "Epoch 57/300\n",
      "1548/1548 [==============================] - 0s 39us/sample - loss: 5.6625 - val_loss: 5.7378\n",
      "Epoch 58/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6621 - val_loss: 5.7348\n",
      "Epoch 59/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6600 - val_loss: 5.7350\n",
      "Epoch 60/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6592 - val_loss: 5.7360\n",
      "Epoch 61/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6581 - val_loss: 5.7330\n",
      "Epoch 62/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6584 - val_loss: 5.7308\n",
      "Epoch 63/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6582 - val_loss: 5.7308\n",
      "Epoch 64/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6570 - val_loss: 5.7313\n",
      "Epoch 65/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6565 - val_loss: 5.7310\n",
      "Epoch 66/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6547 - val_loss: 5.7310\n",
      "Epoch 67/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6550 - val_loss: 5.7283\n",
      "Epoch 68/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.6552 - val_loss: 5.7307\n",
      "Epoch 69/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6539 - val_loss: 5.7303\n",
      "Epoch 70/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6538 - val_loss: 5.7280\n",
      "Epoch 71/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6535 - val_loss: 5.7268\n",
      "Epoch 72/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6543 - val_loss: 5.7282\n",
      "Epoch 73/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6520 - val_loss: 5.7264\n",
      "Epoch 74/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6528 - val_loss: 5.7269\n",
      "Epoch 75/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6522 - val_loss: 5.7260\n",
      "Epoch 76/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6535 - val_loss: 5.7267\n",
      "Epoch 77/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6521 - val_loss: 5.7254\n",
      "Epoch 78/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6537 - val_loss: 5.7268\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548/1548 [==============================] - 0s 57us/sample - loss: 5.6496 - val_loss: 5.7248\n",
      "Epoch 80/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.6503 - val_loss: 5.7246\n",
      "Epoch 81/300\n",
      "1548/1548 [==============================] - 0s 52us/sample - loss: 5.6504 - val_loss: 5.7245\n",
      "Epoch 82/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6505 - val_loss: 5.7260\n",
      "Epoch 83/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6506 - val_loss: 5.7243\n",
      "Epoch 84/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6489 - val_loss: 5.7234\n",
      "Epoch 85/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6502 - val_loss: 5.7235\n",
      "Epoch 86/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6491 - val_loss: 5.7229\n",
      "Epoch 87/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6489 - val_loss: 5.7231\n",
      "Epoch 88/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6495 - val_loss: 5.7238\n",
      "Epoch 89/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6491 - val_loss: 5.7228\n",
      "Epoch 90/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6489 - val_loss: 5.7232\n",
      "Epoch 91/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6481 - val_loss: 5.7232\n",
      "Epoch 92/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6483 - val_loss: 5.7219\n",
      "Epoch 93/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6492 - val_loss: 5.7216\n",
      "Epoch 94/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6483 - val_loss: 5.7216\n",
      "Epoch 95/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6481 - val_loss: 5.7227\n",
      "Epoch 96/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6476 - val_loss: 5.7237\n",
      "Epoch 97/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6462 - val_loss: 5.7215\n",
      "Epoch 98/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6470 - val_loss: 5.7214\n",
      "Epoch 99/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6476 - val_loss: 5.7215\n",
      "Epoch 100/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6456 - val_loss: 5.7224\n",
      "Epoch 101/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6457 - val_loss: 5.7205\n",
      "Epoch 102/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6472 - val_loss: 5.7206\n",
      "Epoch 103/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6458 - val_loss: 5.7204\n",
      "Epoch 104/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6459 - val_loss: 5.7206\n",
      "Epoch 105/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6477 - val_loss: 5.7199\n",
      "Epoch 106/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.6451 - val_loss: 5.7193\n",
      "Epoch 107/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6468 - val_loss: 5.7206\n",
      "Epoch 108/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6469 - val_loss: 5.7206\n",
      "Epoch 109/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6456 - val_loss: 5.7200\n",
      "Epoch 110/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6447 - val_loss: 5.7190\n",
      "Epoch 111/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6455 - val_loss: 5.7196\n",
      "Epoch 112/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6444 - val_loss: 5.7194\n",
      "Epoch 113/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6437 - val_loss: 5.7207\n",
      "Epoch 114/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6459 - val_loss: 5.7185\n",
      "Epoch 115/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6439 - val_loss: 5.7184\n",
      "Epoch 116/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6438 - val_loss: 5.7184\n",
      "Epoch 117/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6440 - val_loss: 5.7177\n",
      "Epoch 118/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6463 - val_loss: 5.7182\n",
      "Epoch 119/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6446 - val_loss: 5.7181\n",
      "Epoch 120/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6455 - val_loss: 5.7194\n",
      "Epoch 121/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6427 - val_loss: 5.7178\n",
      "Epoch 122/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6449 - val_loss: 5.7175\n",
      "Epoch 123/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6449 - val_loss: 5.7186\n",
      "Epoch 124/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6436 - val_loss: 5.7184\n",
      "Epoch 125/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6441 - val_loss: 5.7179\n",
      "Epoch 126/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6427 - val_loss: 5.7176\n",
      "Epoch 127/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6441 - val_loss: 5.7180\n",
      "Epoch 128/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6439 - val_loss: 5.7176\n",
      "Epoch 129/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6429 - val_loss: 5.7178\n",
      "Epoch 130/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6437 - val_loss: 5.7178\n",
      "Epoch 131/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6433 - val_loss: 5.7171\n",
      "Epoch 132/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6420 - val_loss: 5.7171\n",
      "Epoch 133/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6424 - val_loss: 5.7175\n",
      "Epoch 134/300\n",
      "1548/1548 [==============================] - 0s 39us/sample - loss: 5.6443 - val_loss: 5.7172\n",
      "Epoch 135/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6428 - val_loss: 5.7172\n",
      "Epoch 136/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6422 - val_loss: 5.7170\n",
      "Epoch 137/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6421 - val_loss: 5.7177\n",
      "Epoch 138/300\n",
      "1548/1548 [==============================] - 0s 39us/sample - loss: 5.6434 - val_loss: 5.7173\n",
      "Epoch 139/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6417 - val_loss: 5.7173\n",
      "Epoch 140/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6419 - val_loss: 5.7169\n",
      "Epoch 141/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6420 - val_loss: 5.7174\n",
      "Epoch 142/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6412 - val_loss: 5.7169\n",
      "Epoch 143/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6423 - val_loss: 5.7182\n",
      "Epoch 144/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6421 - val_loss: 5.7171\n",
      "Epoch 145/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6415 - val_loss: 5.7178\n",
      "Epoch 146/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6413 - val_loss: 5.7165\n",
      "Epoch 147/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6409 - val_loss: 5.7168\n",
      "Epoch 148/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.6403 - val_loss: 5.7169\n",
      "Epoch 149/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6434 - val_loss: 5.7168\n",
      "Epoch 150/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6413 - val_loss: 5.7172\n",
      "Epoch 151/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6418 - val_loss: 5.7167\n",
      "Epoch 152/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6404 - val_loss: 5.7170\n",
      "Epoch 153/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6394 - val_loss: 5.7168\n",
      "Epoch 154/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6389 - val_loss: 5.7168\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6417 - val_loss: 5.7178\n",
      "Epoch 156/300\n",
      "1376/1548 [=========================>....] - ETA: 0s - loss: 5.6391\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1548/1548 [==============================] - 0s 66us/sample - loss: 5.6411 - val_loss: 5.7172\n",
      "Epoch 157/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6392 - val_loss: 5.7160\n",
      "Epoch 158/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6400 - val_loss: 5.7157\n",
      "Epoch 159/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6386 - val_loss: 5.7156\n",
      "Epoch 160/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6381 - val_loss: 5.7157\n",
      "Epoch 161/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6403 - val_loss: 5.7156\n",
      "Epoch 162/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6386 - val_loss: 5.7155\n",
      "Epoch 163/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6382 - val_loss: 5.7155\n",
      "Epoch 164/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6393 - val_loss: 5.7155\n",
      "Epoch 165/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6394 - val_loss: 5.7156\n",
      "Epoch 166/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6381 - val_loss: 5.7155\n",
      "Epoch 167/300\n",
      "1548/1548 [==============================] - 0s 56us/sample - loss: 5.6387 - val_loss: 5.7155\n",
      "Epoch 168/300\n",
      "1548/1548 [==============================] - 0s 55us/sample - loss: 5.6405 - val_loss: 5.7155\n",
      "Epoch 169/300\n",
      "1548/1548 [==============================] - 0s 55us/sample - loss: 5.6387 - val_loss: 5.7155\n",
      "Epoch 170/300\n",
      "1548/1548 [==============================] - 0s 54us/sample - loss: 5.6383 - val_loss: 5.7154\n",
      "Epoch 171/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6376 - val_loss: 5.7154\n",
      "Epoch 172/300\n",
      "1408/1548 [==========================>...] - ETA: 0s - loss: 5.6319\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6387 - val_loss: 5.7154\n",
      "Epoch 173/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6384 - val_loss: 5.7154\n",
      "Epoch 174/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6380 - val_loss: 5.7154\n",
      "Epoch 175/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6387 - val_loss: 5.7154\n",
      "Epoch 176/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.6392 - val_loss: 5.7154\n",
      "Epoch 177/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6383 - val_loss: 5.7154\n",
      "Epoch 178/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6409 - val_loss: 5.7155\n",
      "Epoch 179/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6384 - val_loss: 5.7155\n",
      "Epoch 180/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6389 - val_loss: 5.7155\n",
      "Epoch 181/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6376 - val_loss: 5.7154\n",
      "Epoch 182/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6376 - val_loss: 5.7154\n",
      "Epoch 183/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6375 - val_loss: 5.7154\n",
      "Epoch 184/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6379 - val_loss: 5.7154\n",
      "Epoch 185/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6383 - val_loss: 5.7154\n",
      "Epoch 186/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 187/300\n",
      "1440/1548 [==========================>...] - ETA: 0s - loss: 5.6345\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6379 - val_loss: 5.7155\n",
      "Epoch 188/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6393 - val_loss: 5.7155\n",
      "Epoch 189/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6397 - val_loss: 5.7155\n",
      "Epoch 190/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6391 - val_loss: 5.7155\n",
      "Epoch 191/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6390 - val_loss: 5.7155\n",
      "Epoch 192/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6377 - val_loss: 5.7155\n",
      "Epoch 193/300\n",
      "1548/1548 [==============================] - 0s 51us/sample - loss: 5.6390 - val_loss: 5.7155\n",
      "Epoch 194/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6377 - val_loss: 5.7155\n",
      "Epoch 195/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6386 - val_loss: 5.7154\n",
      "Epoch 196/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6395 - val_loss: 5.7155\n",
      "Epoch 197/300\n",
      "1408/1548 [==========================>...] - ETA: 0s - loss: 5.6358\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6395 - val_loss: 5.7155\n",
      "Epoch 198/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 199/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 200/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6376 - val_loss: 5.7154\n",
      "Epoch 201/300\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 202/300\n",
      "1548/1548 [==============================] - 0s 48us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 203/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6392 - val_loss: 5.7154\n",
      "Epoch 204/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 205/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6377 - val_loss: 5.7155\n",
      "Epoch 206/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6377 - val_loss: 5.7154\n",
      "Epoch 207/300\n",
      "1440/1548 [==========================>...] - ETA: 0s - loss: 5.6350\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6390 - val_loss: 5.7154\n",
      "Epoch 208/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6386 - val_loss: 5.7155\n",
      "Epoch 209/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6403 - val_loss: 5.7155\n",
      "Epoch 210/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6375 - val_loss: 5.7155\n",
      "Epoch 211/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6389 - val_loss: 5.7155\n",
      "Epoch 212/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6386 - val_loss: 5.7154\n",
      "Epoch 213/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6388 - val_loss: 5.7155\n",
      "Epoch 214/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6397 - val_loss: 5.7155\n",
      "Epoch 215/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 216/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 217/300\n",
      "1376/1548 [=========================>....] - ETA: 0s - loss: 5.6396\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 218/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6394 - val_loss: 5.7154\n",
      "Epoch 219/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6390 - val_loss: 5.7154\n",
      "Epoch 220/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 221/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6372 - val_loss: 5.7154\n",
      "Epoch 222/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6407 - val_loss: 5.7155\n",
      "Epoch 223/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6375 - val_loss: 5.7154\n",
      "Epoch 224/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 225/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6388 - val_loss: 5.7155\n",
      "Epoch 226/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6381 - val_loss: 5.7155\n",
      "Epoch 227/300\n",
      "1440/1548 [==========================>...] - ETA: 0s - loss: 5.6382\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 228/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 229/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6375 - val_loss: 5.7154\n",
      "Epoch 230/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6387 - val_loss: 5.7154\n",
      "Epoch 231/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 232/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6396 - val_loss: 5.7154\n",
      "Epoch 233/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6374 - val_loss: 5.7154\n",
      "Epoch 234/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 235/300\n",
      "1548/1548 [==============================] - 0s 39us/sample - loss: 5.6391 - val_loss: 5.7154\n",
      "Epoch 236/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6397 - val_loss: 5.7154\n",
      "Epoch 237/300\n",
      "1408/1548 [==========================>...] - ETA: 0s - loss: 5.6396\n",
      "Epoch 00237: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6392 - val_loss: 5.7155\n",
      "Epoch 238/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6393 - val_loss: 5.7155\n",
      "Epoch 239/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6378 - val_loss: 5.7154\n",
      "Epoch 240/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6392 - val_loss: 5.7155\n",
      "Epoch 241/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6377 - val_loss: 5.7154\n",
      "Epoch 242/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6376 - val_loss: 5.7155\n",
      "Epoch 243/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6384 - val_loss: 5.7155\n",
      "Epoch 244/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6388 - val_loss: 5.7155\n",
      "Epoch 245/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6380 - val_loss: 5.7154\n",
      "Epoch 246/300\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6379 - val_loss: 5.7154\n",
      "Epoch 247/300\n",
      "1152/1548 [=====================>........] - ETA: 0s - loss: 5.6447\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "1548/1548 [==============================] - 0s 49us/sample - loss: 5.6390 - val_loss: 5.7154\n",
      "Epoch 248/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6374 - val_loss: 5.7154\n",
      "Epoch 249/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 250/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6388 - val_loss: 5.7155\n",
      "Epoch 251/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6382 - val_loss: 5.7155\n",
      "Epoch 252/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6378 - val_loss: 5.7155\n",
      "Epoch 253/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 254/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6374 - val_loss: 5.7154\n",
      "Epoch 255/300\n",
      "1548/1548 [==============================] - 0s 39us/sample - loss: 5.6382 - val_loss: 5.7154\n",
      "Epoch 256/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6374 - val_loss: 5.7154\n",
      "Epoch 257/300\n",
      "1376/1548 [=========================>....] - ETA: 0s - loss: 5.6293\n",
      "Epoch 00257: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6384 - val_loss: 5.7155\n",
      "Epoch 258/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6373 - val_loss: 5.7154\n",
      "Epoch 259/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6381 - val_loss: 5.7155\n",
      "Epoch 260/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6397 - val_loss: 5.7155\n",
      "Epoch 261/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6425 - val_loss: 5.7155\n",
      "Epoch 262/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6395 - val_loss: 5.7156\n",
      "Epoch 263/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6378 - val_loss: 5.7155\n",
      "Epoch 264/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6375 - val_loss: 5.7155\n",
      "Epoch 265/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6382 - val_loss: 5.7155\n",
      "Epoch 266/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6395 - val_loss: 5.7155\n",
      "Epoch 267/300\n",
      "1216/1548 [======================>.......] - ETA: 0s - loss: 5.6506\n",
      "Epoch 00267: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "1548/1548 [==============================] - 0s 45us/sample - loss: 5.6379 - val_loss: 5.7154\n",
      "Epoch 268/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6380 - val_loss: 5.7154\n",
      "Epoch 269/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 270/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6381 - val_loss: 5.7154\n",
      "Epoch 271/300\n",
      "1548/1548 [==============================] - 0s 50us/sample - loss: 5.6395 - val_loss: 5.7155\n",
      "Epoch 272/300\n",
      "1548/1548 [==============================] - 0s 47us/sample - loss: 5.6382 - val_loss: 5.7154\n",
      "Epoch 273/300\n",
      "1548/1548 [==============================] - 0s 46us/sample - loss: 5.6377 - val_loss: 5.7154\n",
      "Epoch 274/300\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6375 - val_loss: 5.7154\n",
      "Epoch 275/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6389 - val_loss: 5.7155\n",
      "Epoch 276/300\n",
      "1548/1548 [==============================] - 0s 43us/sample - loss: 5.6392 - val_loss: 5.7155\n",
      "Epoch 277/300\n",
      "1344/1548 [=========================>....] - ETA: 0s - loss: 5.6382\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "1548/1548 [==============================] - 0s 44us/sample - loss: 5.6384 - val_loss: 5.7155\n",
      "Epoch 278/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6385 - val_loss: 5.7154\n",
      "Epoch 279/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6372 - val_loss: 5.7154\n",
      "Epoch 280/300\n",
      "1548/1548 [==============================] - 0s 40us/sample - loss: 5.6380 - val_loss: 5.7154\n",
      "Epoch 281/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6374 - val_loss: 5.7154\n",
      "Epoch 282/300\n",
      "1548/1548 [==============================] - 0s 42us/sample - loss: 5.6383 - val_loss: 5.7154\n",
      "Epoch 283/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6380 - val_loss: 5.7154\n",
      "Epoch 284/300\n",
      "1548/1548 [==============================] - 0s 41us/sample - loss: 5.6388 - val_loss: 5.7154\n",
      "Epoch 00284: early stopping\n"
     ]
    }
   ],
   "source": [
    "res = dca(adata,\n",
    "          ae_type='meth-encoder-constant',\n",
    "          return_info=True,\n",
    "          return_model=True,\n",
    "          return_bottleneck=True,\n",
    "          verbose=True,\n",
    "          early_stop=50,\n",
    "          epochs=300,\n",
    "          network_kwds={'debug': True},\n",
    "          init='glorot_normal',\n",
    "          hidden_size=(16,8,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8237a8",
   "metadata": {},
   "source": [
    "### Writing the data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#di = '/users/lvelten/project/Methylome/analysis/missionbio/tapestri/' + sample + '/methylation_autoencoder/'\n",
    "di = '/users/lvelten/project/Methylome/analysis/missionbio/downsampling/Sample5/percent_1/methylation_autoencoder/'\n",
    "if not os.path.isdir(di):\n",
    "    os.mkdir(di)\n",
    "    \n",
    "pd.DataFrame(adata.obsm['X_meth_value']).to_csv(di + 'mixture_prob_dca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb56ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata.obsm['X_bottleneck']).to_csv(di + 'bottleneck.csv')\n",
    "pd.DataFrame(adata.obsm['X_enzyme_activity']).to_csv(di + 'enzyme.csv')\n",
    "pd.DataFrame(adata.obsm['mean1_norm']).to_csv(di + 'mean1_norm.csv')\n",
    "pd.DataFrame(adata.obsm['alpha']).to_csv(di + 'alpha.csv')\n",
    "pd.DataFrame(adata.obs.size_factors).to_csv(di + 'size_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c5afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata.var['meth_dispersion1']).to_csv(di + 'meth_dispersion1.csv')\n",
    "pd.DataFrame(adata.var['meth_dispersion2']).to_csv(di + 'meth_dispersion2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2377f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2264202"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obsm['X_enzyme_activity'].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
